{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "text ='''The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in\n",
        "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel'''\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract Company Names using NER\n",
        "company_names = [ent.text for ent in doc.ents if ent.label_ == 'ORG']\n",
        "company_names_count = len(company_names)\n",
        "\n",
        "print(\"Company Names:\", company_names)\n",
        "print(\"Count:\", company_names_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3fqA8L0y4UD",
        "outputId": "c66ad1b1-5553-41af-8764-7f336846deaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Company Names: ['Tesla', 'Walmart', 'Amazon', 'Microsoft', 'Google', 'Infosys', 'Reliance', 'HDFC Bank', 'Hindustan Unilever', 'Bharti']\n",
            "Count: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Create a blank Spacy pipeline\n",
        "nlp_blank = spacy.blank(\"en\")\n",
        "\n",
        "# Check the pipeline components and names\n",
        "print(\"Blank pipeline components:\", nlp_blank.pipe_names)\n",
        "\n",
        "# Process text with the blank pipeline\n",
        "doc_blank = nlp_blank(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "print(\"Tokens from the blank pipeline:\")\n",
        "for token in doc_blank:\n",
        "    print(token.text, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)\n",
        "\n",
        "# Load the pre-trained pipeline\n",
        "nlp_pretrained = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Check the pipeline components and names for the pre-trained pipeline\n",
        "print(\"\\nPre-trained pipeline components:\", nlp_pretrained.pipe_names)\n",
        "\n",
        "# Process text with the pre-trained pipeline\n",
        "doc_pretrained = nlp_pretrained(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "print(\"\\nTokens from the pre-trained pipeline:\")\n",
        "for token in doc_pretrained:\n",
        "    print(token.text, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g6eWyAXzo_z",
        "outputId": "e24cc150-ae1f-4c5e-cc89-bdc8880b42a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blank pipeline components: []\n",
            "Tokens from the blank pipeline:\n",
            "Captain  |  None  |  \n",
            "america  |  None  |  \n",
            "ate  |  None  |  \n",
            "100  |  None  |  \n",
            "$  |  None  |  \n",
            "of  |  None  |  \n",
            "samosa  |  None  |  \n",
            ".  |  None  |  \n",
            "Then  |  None  |  \n",
            "he  |  None  |  \n",
            "said  |  None  |  \n",
            "I  |  None  |  \n",
            "can  |  None  |  \n",
            "do  |  None  |  \n",
            "this  |  None  |  \n",
            "all  |  None  |  \n",
            "day  |  None  |  \n",
            ".  |  None  |  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/glossary.py:20: UserWarning: [W118] Term '' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pre-trained pipeline components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "\n",
            "Tokens from the pre-trained pipeline:\n",
            "Captain  |  proper noun  |  Captain\n",
            "america  |  proper noun  |  america\n",
            "ate  |  verb  |  eat\n",
            "100  |  numeral  |  100\n",
            "$  |  numeral  |  $\n",
            "of  |  adposition  |  of\n",
            "samosa  |  proper noun  |  samosa\n",
            ".  |  punctuation  |  .\n",
            "Then  |  adverb  |  then\n",
            "he  |  pronoun  |  he\n",
            "said  |  verb  |  say\n",
            "I  |  pronoun  |  I\n",
            "can  |  auxiliary  |  can\n",
            "do  |  verb  |  do\n",
            "this  |  pronoun  |  this\n",
            "all  |  determiner  |  all\n",
            "day  |  noun  |  day\n",
            ".  |  punctuation  |  .\n"
          ]
        }
      ]
    }
  ]
}